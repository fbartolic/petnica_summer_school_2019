{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# To render plots inside the Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll learn how to fit a mock light curve of a transiting exoplanet\n",
    "using a method for sampling the posterior probability distribution over the \n",
    "model parameters. The notebook is based on [these](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.716.7757&rep=rep1&type=pdf) excellent lecture notes on Bayesian statistics by\n",
    "Brendon Brewer which I highly recommend.\n",
    "\n",
    "The goal is to sample the posterior probability distribution over the\n",
    "model parameters, conditioned on the data:\n",
    "\n",
    "$$p(\\boldsymbol \\theta |\\mathbf{D})=\\frac{p(\\mathbf{D}|\\boldsymbol\\theta)\\,p(\\boldsymbol\\theta)}{p(\\mathbf{D})}$$\n",
    "\n",
    "The most popular class of algorithms which  used for generating samples from a pdf \n",
    "are called *Markov Chain Monte Carlo* algorithms or MCMC for short.\n",
    "The simplest of these algorithms, and the very first one that was developed, is \n",
    "called the *Metropolis algorithm*:\n",
    "\n",
    "**The Metropolis algorithm**\n",
    "\n",
    "Given a most recent sample $\\boldsymbol\\theta^{(k)}$, to generate the next sample\n",
    "- Draw a proposal $\\boldsymbol\\theta'$ from a proposal pdf \n",
    " $q(\\boldsymbol\\theta'\\lvert\\boldsymbol\\theta^{(k)})$ which *we know how to sample from* (i.e. \n",
    "there's a simple numpy function that does it)\n",
    "- Draw a uniform random number $r\\sim\\mathcal{U}(0,1)$\n",
    "- If $r\\leq p(\\boldsymbol\\theta')/p(\\boldsymbol\\theta^{(k)})$ then \n",
    "    $\\boldsymbol\\theta^{(k+1)}\\leftarrow\\boldsymbol\\theta'$ (accept proposal), else \n",
    "    $\\boldsymbol\\theta^{(k+1)}\\leftarrow\\boldsymbol\\theta^{(k)}$ (reject proposal)\n",
    "- Repeat until happy with the number of samples\n",
    "        \n",
    "The main characteristic of this algorithm is that it defines a biased random walk\n",
    "through\n",
    "the parameter space in such a way that we end up sampling the target pdf $p(\\boldsymbol\\theta)$.\n",
    "As the number of samples approaches infinity, we are guaranteed that any sensible expectation\n",
    "value under the pdf can be approximated by a sum over the drawn samples:\n",
    "\n",
    "$$    \\lim_{K\\rightarrow\\infty}\\frac{1}{K}\\sum_{i=1}^Kg(\n",
    "    \\theta^{(k)})\\rightarrow\\mathbb{E}_{p(\\theta)}\\left[g(\\theta)\\right]\n",
    "$$\n",
    "\n",
    "The reason why the Metropolis algorithm works is that it defines a process called a \n",
    "*Markov chain* in which the probability of moving from a current state to \n",
    "a successive state in the chain depends only on the current state and not any\n",
    "past states.\n",
    "Markov chains have stationary probability distributions over states and the stationary\n",
    "distribution for the Markov chain defined by the Metropolis algorithm turns out to be\n",
    "the target distribution $p(\\boldsymbol\\theta)$.\n",
    "In order for the above to hold, the proposal pdf $q$ must satisfied a property\n",
    "called *detailed balance*, defined by\n",
    "\n",
    "$$\n",
    "q(\\boldsymbol\\theta'\\lvert\\boldsymbol\\theta)=q(\\boldsymbol\\theta\\lvert\\boldsymbol\\theta')\n",
    "$$\n",
    "\n",
    "that is, it has to be reversible in the sense that the probability of transitioning from $\\theta$ \n",
    "to $\\theta'$ is the same as the probability of transitioning from $\\theta'$ to $\\theta$\n",
    "for any two pairs of states $\\theta$ and $\\theta'$.\n",
    "\n",
    "The simplest choice for the proposal pdf is a multivariate Gaussian distribution \n",
    "for $\\boldsymbol\\theta'$ with a mean $\\boldsymbol\\theta$ and a covariance matrix $\\boldsymbol\\Sigma$, or\n",
    "in shorthand notation, $q(\\boldsymbol\\theta'\\lvert\\boldsymbol\\theta)\\sim\\mathcal{N}(\\boldsymbol\\theta,\\Sigma)$\n",
    "(`np.random.multivariate_normal` in `numpy`).\n",
    "The variances in the covariance matrix \n",
    "then define a characteristic step size in the parameter space for each parameter, if the step size is\n",
    "too large most proposals will get rejected and the chain might miss regions of high \n",
    "probability. \n",
    "If on the other hand the step size is too small, most steps are likely to be accepted\n",
    "but we might not explore the parameter space fully.\n",
    "\n",
    "All MCMC algorithms have the same goal, namely, generating samples from a target\n",
    "pdf such that the least number of samples $K$ are needed to accurately approximate\n",
    "the expectation integral.\n",
    "Some metrics on which MCMC samplers are judged upon are\n",
    "- Efficiency - what is the acceptance rate of the proposals?\n",
    "- Coverage - is the target pdf explored completely?\n",
    "- Correlation - how correlated are individual samples with past samples?\n",
    "- High dimensions - can the sampler handle high dimensional spaces?\n",
    "- Derivatives - does the sampler require the derivatives of the likelihood with respect \n",
    "to the model parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a mock transit event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a physical model for modeling the exoplanet transit, a model which would have to include\n",
    "parameters such as the period of the planet, the ratio of its radius to the radius of the star, the impact\n",
    "parameter, the eccentricity of the orbit and other physics, we'll use a mock transit model specified\n",
    "by a trivial piecwise function defined as follows:\n",
    "\n",
    "$$\n",
    "\\mu(t)=\\left\\{\\begin{array}{ll}{A,} & {\\left(t_{c}-w / 2\\right) \\leq t \\leq\\left(t_{c}+w / 2\\right)} \\\\ {A-b,} & {\\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "where $A$ is the stellar flux outside of the transit, $b$ is the depth of the transit, $t_c$ is the\n",
    "time at midpoint and $w$ is the width of the transit.\n",
    "The inference procedure is exactly the same using a more realistic transit\n",
    "model except the function we plug into the likelihood is different and a lot more complex.\n",
    "\n",
    "First, we'll generate  mock data using the *same model we'll use for fitting*, this will allow us\n",
    "to test whether our MCMC sampler is working correctly and recovering the true parameters. Of course, in reality\n",
    "we never know the true model which generated our data, it is usually far more complex than the models we\n",
    "use to fit the data.\n",
    "\n",
    "We'll take the true parameters to be:\n",
    "\n",
    "$$\\boldsymbol\\theta=\\{A,b,t_c,w\\}=\\{1.0, 0.5,4.0, 3.0\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mock data\n",
    "def mean_model(t, params):\n",
    "    A, b, tc, w = params\n",
    "    \n",
    "    mu = A*np.ones(len(t))\n",
    "    mask = (t > tc - w/2) & (t < tc + w/2)\n",
    "    mu[mask] = A - b\n",
    "    \n",
    "    return mu\n",
    "\n",
    "t = np.linspace(0, 10, 200)\n",
    "mean_flux = mean_model(t, [1., 0.5, 4., 3.])\n",
    "\n",
    "# Add gaussian noise\n",
    "covariance_matrix = 0.2**2*np.diag(np.ones(len(t)))\n",
    "sigma = np.sqrt(np.diagonal(covariance_matrix))\n",
    "mock_flux = np.random.multivariate_normal(mean_flux, covariance_matrix)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.errorbar(t, mock_flux, sigma, \n",
    "            fmt='o', color='black', alpha=0.5, label='mock data')\n",
    "ax.plot(t, mean_flux, color='C1', lw=2., label='mean model')\n",
    "ax.grid()\n",
    "ax.set_ylabel('flux [arbitrary units]')\n",
    "ax.set_xlabel('time [arbitrary units]')\n",
    "ax.legend(prop={'size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's pretend we don't know the true parameters and fit a model using MCMC.\n",
    "We'll assume that the likelihood $p(\\mathbf{D}|\\boldsymbol\\theta)$ is Gaussian and that all the data \n",
    "points independently distributed with identical uncertainties $\\sigma_i\\equiv\\sigma$ and those are known. \n",
    "The likelihood for the *i-th* data point is then\n",
    "\n",
    "$$\n",
    "p(D_i|\\boldsymbol\\theta)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(D_n-\\mu(t_n))^2}{2\\sigma^2}\\right]\n",
    "$$\n",
    "\n",
    "and since we assume independance between data points, the complete likelihood is just the product\n",
    "of the above factors\n",
    "\n",
    "$$\n",
    "p(\\mathbf{D}|\\boldsymbol\\theta)=\\prod_{n=1}^Np(D_n|\\boldsymbol\\theta)=\\prod_{n=1}^N\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(D_n-\\mu(t_n))^2}{2\\sigma^2}\\right]\n",
    "$$\n",
    "\n",
    "and the log likelihood is\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathbf{D}|\\boldsymbol\\theta)= -\\frac{1}{2}N\\ln(2\\pi\\sigma^2)-\\sum_{n=1}^N\n",
    "\\frac{(D_n-\\mu_n)^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "We assume an independent prior over the model parameters, that is\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol\\theta)=p(A,b,t_c,w)=p(A)\\,p(b)\\,p(t_c)\\,p(w)\n",
    "$$\n",
    "\n",
    "and we take all priors to be [uniform](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous))\n",
    "in some range.\n",
    "\n",
    "In the code, we need to implement the following functions: `log_prior`, `log_likelihood`, `log_posterior` and `sample`.\n",
    "For now, we'll use uniform priors for all of the parameters, this is generally a bad idea but it won't matter\n",
    "much in this example. The priors should return `-np.inf` outside of the parameter regions of interest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "- Read through this notebook!!!\n",
    "- Complete the missing bits of code for the metropolis sampler, sample the posterior for 10000 steps, check that the acceptance rate is reasonable\n",
    "- Plot the chains, visually check if they converged\n",
    "- Plot histograms of the posterior samples for a parameter of your choice\n",
    "- Plot samples from the model in data space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(params):\n",
    "    \"\"\"\n",
    "    Computes the natural logarithm of the join prior distribution\n",
    "    over all of the model parameters ln_p(theta1, ..., theta_N).\n",
    "    We assume an independent uniform prior over all parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list\n",
    "        List containing model parameters: [A, B, tc, w]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Natural logarithm of the joint prior pdf.\n",
    "    \"\"\"\n",
    "    # Unpack the parameters\n",
    "    A, b, tc, w  = params\n",
    "    \n",
    "    # Uniform priors for all parameters\n",
    "    if (A < 0) or (A > 2.):\n",
    "        return -np.inf # negative infinity\n",
    "    elif (b < 0) or (b > 1.):\n",
    "        return -np.inf \n",
    "    elif (tc < 0) or (tc > 10.):\n",
    "        return -np.inf \n",
    "    elif (w < 0) or (w > 10.):\n",
    "        return -np.inf \n",
    "    \n",
    "    # If all the parameters are within the bounds, return a constant. \n",
    "    # In principle we should return the log-pdf but for a uniform pdf\n",
    "    # that's just a constant factor and MCMC doesn't care about \n",
    "    # normalization\n",
    "    else:\n",
    "        return 0. \n",
    "    \n",
    "def log_likelihood(params, t, flux, sigma):\n",
    "    \"\"\"\n",
    "    Computes the natural logarithm of the (Gaussian) likelihood \n",
    "    function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list\n",
    "        List containing model parameters: [A, B, tc, w]\n",
    "    t : ndarray\n",
    "        Numpy array of times at which observations were taken\n",
    "    flux : ndarray\n",
    "        Numpy array of fluxes observed at those times. Needs to have\n",
    "        the same shape as t. \n",
    "    sigma : ndarray\n",
    "        Numpy array containing the uncertainties assoiciated with each \n",
    "        data point (so called \"error bars\").\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Natural logarithm of the likelihood.\n",
    "    \"\"\"\n",
    "    # Unpack the parameters\n",
    "    A, b, tc, w  = params\n",
    "    \n",
    "    N = float(len(t)) # nr of data points\n",
    "    \n",
    "    # Compute the mean model for given parameters\n",
    "    prediction = mean_model(t, params)\n",
    "    \n",
    "    # Compute residuals with respect to mean model\n",
    "    r = flux - prediction\n",
    "        \n",
    "    # Compute the log likelihood\n",
    "    ln_L = -0.5*N*np.sqrt(2*np.pi*sigma[0]) - 0.5*np.sum(r**2/sigma**2)\n",
    "        \n",
    "    return ln_L \n",
    "    \n",
    "def log_posterior(params, t, flux, sigma):\n",
    "    \"\"\"\n",
    "    Computes the natural logarithm of the posterior pdf.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list\n",
    "        List containing model parameters: [A, B, tc, w]\n",
    "    t : ndarray\n",
    "        Numpy array of times at which observations were taken\n",
    "    flux : ndarray\n",
    "        Numpy array of fluxes observed at those times. Needs to have\n",
    "        the same shape as t. \n",
    "    sigma : ndarray\n",
    "        Numpy array containing the uncertainties assoiciated with each \n",
    "        data point (so called \"error bars\").\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Natural logarithm of the posterior pdf.\n",
    "    \"\"\"\n",
    "    ln_prior = log_prior(params)\n",
    "    \n",
    "    if np.isinf(ln_prior):\n",
    "        return ln_prior\n",
    "    else:           \n",
    "        ln_likelihood = log_likelihood(params, t, flux, sigma)\n",
    "        return ln_prior + ln_likelihood\n",
    "    \n",
    "def proposal_distribution(params, sigmas):\n",
    "    \"\"\"\n",
    "    Returns a sample from the proposal distribution q(theta'|theta).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list\n",
    "        List containing model parameters: [A, B, tc, w]\n",
    "    sigmas : list\n",
    "        Array of size len(params) containing the diagonal elements \n",
    "        square roots of the variances on the diagonal in a multivariate\n",
    "        gaussian distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Natural logarithm of the likelihood.\n",
    "    \"\"\"\n",
    "    # Generate a proposal from a multivariate gaussian\n",
    "    step = np.random.multivariate_normal(np.zeros(len(params)),\n",
    "                                        np.diag(sigmas))\n",
    "\n",
    "    return params + step\n",
    "    \n",
    "def sample(initial_params, n_steps, sigmas):\n",
    "    \"\"\"\n",
    "    Uses the Metropolis algorithm to sample the posterior pdf.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    initial_params : list\n",
    "        List containing initial values of model parameters: [A, B, tc, w]\n",
    "    n_steps : int\n",
    "        Number of sampling steps we want to use.\n",
    "    sigmas\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Array of shape (n_steps, n_params) containing the posterior samples.\n",
    "    \"\"\"\n",
    "    # Evaluate the log posterior for the inital parameters\n",
    "    logp = log_posterior(initial_params, t, mock_flux, sigma)\n",
    "        \n",
    "    # Create an array for storing the samples\n",
    "    samples = np.zeros((n_steps, len(initial_params)))\n",
    "    \n",
    "    params = initial_params\n",
    "    \n",
    "    # Keep track of the number of accepted vs. rejected steps\n",
    "    n_accepted = 0\n",
    "    \n",
    "    # Main loop\n",
    "    for i in range(n_steps): \n",
    "        # Draw new parameters from a proposal distribution\n",
    "        new_params = proposal_distribution(params, sigmas)\n",
    "                \n",
    "        # Evaluate the posterior for the proposal\n",
    "        logp_new = log_posterior(new_params, t, mock_flux, sigma)\n",
    "            \n",
    "        # Accept or reject the proposal\n",
    "        r = np.random.rand() # Uniform number r ~ U(0, 1)\n",
    "        \n",
    "        ## YOUR CODE GOES HERE\n",
    "            \n",
    "        \n",
    "    print(\"Acceptance rate: \", float(n_accepted)/float(n_steps))\n",
    "            \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the initial position of the chain (initial parameters)\n",
    "initial_params = [0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "sigmas = [0.0005, 0.0005, 0.005, 0.005]\n",
    "\n",
    "# Call the sample function\n",
    "samples = sample(initial_params, 10000, sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].plot(samples[:, i], color='black')\n",
    "    ax[i].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot posterior samples in data space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.errorbar(t, mock_flux, sigma, \n",
    "            fmt='o', color='black', alpha=0.5)\n",
    "ax.grid()\n",
    "ax.set_ylabel('flux [arbitrary units]')\n",
    "ax.set_xlabel('time [arbitrary units]')\n",
    "ax.legend(prop={'size': 16})\n",
    "\n",
    "\n",
    "for params in samples[np.random.randint(len(samples), size=100)]:\n",
    "    model_prediction = mean_model(t, params)\n",
    "    ax.plot(t, model_prediction, color='C1', label='mean model', alpha=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
